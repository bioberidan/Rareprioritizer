{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Batch Processing - Cookbook Example 03\n",
        "\n",
        "This notebook demonstrates efficient batch processing of multiple diseases using WebSearcher agents with concurrency and optimization techniques.\n",
        "\n",
        "## üéØ What You'll Learn\n",
        "\n",
        "- Concurrent processing of multiple diseases\n",
        "- Progress tracking and monitoring\n",
        "- Results aggregation and ranking\n",
        "- Memory-efficient batch processing\n",
        "- Performance optimization strategies\n",
        "- Export capabilities for research teams\n",
        "\n",
        "## ‚ö° Batch Processing Benefits\n",
        "\n",
        "1. **Efficiency**: Process dozens of diseases simultaneously\n",
        "2. **Scalability**: Handle large disease portfolios  \n",
        "3. **Monitoring**: Real-time progress and error tracking\n",
        "4. **Results Management**: Automated aggregation and ranking\n",
        "5. **Export Ready**: CSV/JSON output for research teams\n",
        "\n",
        "Let's build a high-performance batch analysis system! üöÄ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup for batch processing\n",
        "import sys\n",
        "import os\n",
        "sys.path.insert(0, os.path.abspath('../../'))\n",
        "\n",
        "# Initialize prompt system\n",
        "import apps.research_prioritization.prompts.prompt_registry\n",
        "from agents import WebSearcher\n",
        "\n",
        "# Imports for batch processing\n",
        "import concurrent.futures\n",
        "import time\n",
        "import json\n",
        "import pandas as pd\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import Dict, List, Any, Optional\n",
        "from datetime import datetime\n",
        "\n",
        "# Enhanced client configuration for batch processing\n",
        "BATCH_CLIENT_CONFIG = {\n",
        "    \"reasoning\": {\"effort\": \"low\"},  # Faster processing for batch\n",
        "    \"max_output_tokens\": 3000        # Reduced for efficiency\n",
        "}\n",
        "\n",
        "print(\"‚ö° Batch Processing System Ready!\")\n",
        "print(f\"üíª Optimized Configuration: {BATCH_CLIENT_CONFIG}\")\n",
        "print(f\"üîó Ready for concurrent disease analysis\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Batch processing framework with progress tracking\n",
        "@dataclass\n",
        "class BatchResult:\n",
        "    \"\"\"Container for batch processing results\"\"\"\n",
        "    orphacode: str\n",
        "    disease_name: str\n",
        "    socioeconomic_score: Optional[int] = None\n",
        "    groups_found: int = 0\n",
        "    priority_score: int = 0\n",
        "    processing_time: float = 0.0\n",
        "    status: str = \"pending\"  # pending, completed, failed\n",
        "    error_message: Optional[str] = None\n",
        "    timestamp: Optional[str] = None\n",
        "    \n",
        "    def to_dict(self) -> Dict[str, Any]:\n",
        "        return asdict(self)\n",
        "\n",
        "class BatchProgress:\n",
        "    \"\"\"Progress tracking for batch operations\"\"\"\n",
        "    def __init__(self, total_diseases: int):\n",
        "        self.total = total_diseases\n",
        "        self.completed = 0\n",
        "        self.failed = 0\n",
        "        self.start_time = time.time()\n",
        "        \n",
        "    def update(self, success: bool = True):\n",
        "        if success:\n",
        "            self.completed += 1\n",
        "        else:\n",
        "            self.failed += 1\n",
        "    \n",
        "    def get_progress(self) -> Dict[str, Any]:\n",
        "        elapsed = time.time() - self.start_time\n",
        "        processed = self.completed + self.failed\n",
        "        remaining = self.total - processed\n",
        "        \n",
        "        if processed > 0:\n",
        "            avg_time = elapsed / processed\n",
        "            eta_seconds = avg_time * remaining\n",
        "        else:\n",
        "            eta_seconds = 0\n",
        "        \n",
        "        return {\n",
        "            \"processed\": processed,\n",
        "            \"completed\": self.completed,\n",
        "            \"failed\": self.failed,\n",
        "            \"total\": self.total,\n",
        "            \"progress_pct\": (processed / self.total) * 100 if self.total > 0 else 0,\n",
        "            \"elapsed_time\": elapsed,\n",
        "            \"eta_seconds\": eta_seconds,\n",
        "            \"success_rate\": (self.completed / processed) * 100 if processed > 0 else 0\n",
        "        }\n",
        "    \n",
        "    def print_status(self):\n",
        "        progress = self.get_progress()\n",
        "        print(f\"üìä Progress: {progress['processed']}/{self.total} ({progress['progress_pct']:.1f}%)\")\n",
        "        print(f\"‚úÖ Completed: {progress['completed']} | ‚ùå Failed: {progress['failed']}\")\n",
        "        print(f\"‚è±Ô∏è  Elapsed: {progress['elapsed_time']:.1f}s | ETA: {progress['eta_seconds']:.1f}s\")\n",
        "        print(f\"üìà Success Rate: {progress['success_rate']:.1f}%\")\n",
        "\n",
        "class BatchProcessor:\n",
        "    \"\"\"High-performance batch processor for disease analysis\"\"\"\n",
        "    \n",
        "    def __init__(self, client_config: dict, max_workers: int = 3):\n",
        "        self.client_config = client_config\n",
        "        self.max_workers = max_workers\n",
        "        self.socio_searcher = WebSearcher(\"socioeconomic_v2\", client_config)\n",
        "        self.groups_searcher = WebSearcher(\"groups_v1\", client_config)\n",
        "        \n",
        "    def process_single_disease(self, orphacode: str, disease_name: str) -> BatchResult:\n",
        "        \"\"\"Process a single disease with error handling and timing\"\"\"\n",
        "        start_time = time.time()\n",
        "        result = BatchResult(\n",
        "            orphacode=orphacode,\n",
        "            disease_name=disease_name,\n",
        "            timestamp=datetime.now().isoformat()\n",
        "        )\n",
        "        \n",
        "        try:\n",
        "            template_data = {\"orphacode\": orphacode, \"disease_name\": disease_name}\n",
        "            \n",
        "            # Socioeconomic analysis\n",
        "            try:\n",
        "                socio_response = self.socio_searcher.search(template_data)\n",
        "                result.socioeconomic_score = int(socio_response.score)\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è  Socioeconomic analysis failed for {disease_name}: {str(e)[:50]}...\")\n",
        "            \n",
        "            # Groups analysis  \n",
        "            try:\n",
        "                groups_response = self.groups_searcher.search(template_data)\n",
        "                result.groups_found = len(groups_response.groups) if groups_response.groups else 0\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è  Groups analysis failed for {disease_name}: {str(e)[:50]}...\")\n",
        "            \n",
        "            # Calculate priority score\n",
        "            if result.socioeconomic_score is not None:\n",
        "                result.priority_score = result.socioeconomic_score\n",
        "                if result.groups_found > 0:\n",
        "                    result.priority_score += 2  # Boost for existing research\n",
        "                result.priority_score = min(result.priority_score, 10)\n",
        "            \n",
        "            result.status = \"completed\"\n",
        "            \n",
        "        except Exception as e:\n",
        "            result.status = \"failed\"\n",
        "            result.error_message = str(e)\n",
        "            print(f\"‚ùå Complete failure for {disease_name}: {str(e)[:50]}...\")\n",
        "        \n",
        "        result.processing_time = time.time() - start_time\n",
        "        return result\n",
        "    \n",
        "    def process_batch(self, diseases: List[tuple], progress_updates: bool = True) -> List[BatchResult]:\n",
        "        \"\"\"Process multiple diseases concurrently with progress tracking\"\"\"\n",
        "        results = []\n",
        "        progress = BatchProgress(len(diseases))\n",
        "        \n",
        "        print(f\"üöÄ Starting batch processing of {len(diseases)} diseases\")\n",
        "        print(f\"‚öôÔ∏è  Max workers: {self.max_workers}\")\n",
        "        \n",
        "        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
        "            # Submit all tasks\n",
        "            future_to_disease = {\n",
        "                executor.submit(self.process_single_disease, orphacode, name): (orphacode, name)\n",
        "                for orphacode, name in diseases\n",
        "            }\n",
        "            \n",
        "            # Collect results as they complete\n",
        "            for future in concurrent.futures.as_completed(future_to_disease):\n",
        "                orphacode, name = future_to_disease[future]\n",
        "                try:\n",
        "                    result = future.result()\n",
        "                    results.append(result)\n",
        "                    progress.update(success=(result.status == \"completed\"))\n",
        "                    \n",
        "                    if progress_updates and (progress.completed + progress.failed) % 2 == 0:\n",
        "                        print(f\"‚úÖ Completed: {name} (Score: {result.priority_score})\")\n",
        "                        \n",
        "                except Exception as e:\n",
        "                    # This shouldn't happen as errors are handled in process_single_disease\n",
        "                    failed_result = BatchResult(\n",
        "                        orphacode=orphacode,\n",
        "                        disease_name=name,\n",
        "                        status=\"failed\",\n",
        "                        error_message=str(e),\n",
        "                        timestamp=datetime.now().isoformat()\n",
        "                    )\n",
        "                    results.append(failed_result)\n",
        "                    progress.update(success=False)\n",
        "                    print(f\"‚ùå Critical failure: {name}\")\n",
        "        \n",
        "        print(\"\\nüéØ BATCH PROCESSING COMPLETE!\")\n",
        "        progress.print_status()\n",
        "        return results\n",
        "\n",
        "# Initialize batch processor\n",
        "batch_processor = BatchProcessor(BATCH_CLIENT_CONFIG, max_workers=3)\n",
        "print(\"üè≠ Batch processor initialized with concurrent execution capability!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## üß¨ Example Batch: Neurological Rare Diseases\n",
        "\n",
        "Let's process a representative batch of neurological rare diseases to demonstrate batch processing capabilities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define batch of neurological diseases for processing\n",
        "neurological_diseases = [\n",
        "    (\"905\", \"Wilson disease\"),\n",
        "    (\"399\", \"Huntington disease\"),  \n",
        "    (\"98\", \"Alpers syndrome\"),\n",
        "    (\"1175\", \"Pelizaeus-Merzbacher disease\"),\n",
        "    (\"289\", \"Early-onset primary dystonia\"),\n",
        "    (\"447\", \"Late-onset metachromatic leukodystrophy\")\n",
        "]\n",
        "\n",
        "print(f\"üß† Neurological Diseases Batch: {len(neurological_diseases)} diseases\")\n",
        "for i, (code, name) in enumerate(neurological_diseases, 1):\n",
        "    print(f\"  {i}. {name} (Orphacode: {code})\")\n",
        "\n",
        "print(\"\\nüöÄ Starting batch processing...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Execute batch processing\n",
        "batch_results = batch_processor.process_batch(neurological_diseases, progress_updates=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze and rank batch results\n",
        "print(\"\\nüìä BATCH RESULTS ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Sort results by priority score (descending)\n",
        "successful_results = [r for r in batch_results if r.status == \"completed\" and r.priority_score > 0]\n",
        "failed_results = [r for r in batch_results if r.status == \"failed\"]\n",
        "partial_results = [r for r in batch_results if r.status == \"completed\" and r.priority_score == 0]\n",
        "\n",
        "successful_results.sort(key=lambda x: x.priority_score, reverse=True)\n",
        "\n",
        "print(f\"‚úÖ Successful analyses: {len(successful_results)}\")\n",
        "print(f\"‚ùå Failed analyses: {len(failed_results)}\")  \n",
        "print(f\"‚ö†Ô∏è  Partial results: {len(partial_results)}\")\n",
        "\n",
        "if successful_results:\n",
        "    print(f\"\\nüèÜ TOP PRIORITY DISEASES:\")\n",
        "    print(f\"{'Rank':<4} {'Disease':<30} {'Priority':<8} {'Socio':<6} {'Groups':<7} {'Time(s)':<8}\")\n",
        "    print(\"-\" * 70)\n",
        "    \n",
        "    for i, result in enumerate(successful_results[:5], 1):\n",
        "        print(f\"{i:<4} {result.disease_name[:28]:<30} {result.priority_score:<8} \"\n",
        "              f\"{result.socioeconomic_score or 'N/A':<6} {result.groups_found:<7} \"\n",
        "              f\"{result.processing_time:.1f}s\")\n",
        "\n",
        "if failed_results:\n",
        "    print(f\"\\n‚ùå FAILED ANALYSES:\")\n",
        "    for result in failed_results:\n",
        "        error_short = result.error_message[:40] + \"...\" if result.error_message and len(result.error_message) > 40 else result.error_message\n",
        "        print(f\"  ‚Ä¢ {result.disease_name}: {error_short}\")\n",
        "\n",
        "# Performance metrics\n",
        "if batch_results:\n",
        "    total_time = sum(r.processing_time for r in batch_results)\n",
        "    avg_time = total_time / len(batch_results)\n",
        "    max_time = max(r.processing_time for r in batch_results)\n",
        "    min_time = min(r.processing_time for r in batch_results)\n",
        "    \n",
        "    print(f\"\\n‚è±Ô∏è  PERFORMANCE METRICS:\")\n",
        "    print(f\"  Total processing time: {total_time:.1f}s\")\n",
        "    print(f\"  Average per disease: {avg_time:.1f}s\")\n",
        "    print(f\"  Fastest analysis: {min_time:.1f}s\")\n",
        "    print(f\"  Slowest analysis: {max_time:.1f}s\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export results for research teams\n",
        "def export_batch_results(results: List[BatchResult], format: str = \"csv\") -> str:\n",
        "    \"\"\"Export batch results to CSV or JSON format\"\"\"\n",
        "    \n",
        "    # Convert to list of dictionaries\n",
        "    data = [result.to_dict() for result in results]\n",
        "    \n",
        "    if format.lower() == \"csv\":\n",
        "        # Create DataFrame and export to CSV\n",
        "        df = pd.DataFrame(data)\n",
        "        # Sort by priority score descending\n",
        "        df = df.sort_values('priority_score', ascending=False)\n",
        "        \n",
        "        filename = f\"batch_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
        "        df.to_csv(filename, index=False)\n",
        "        \n",
        "        print(f\"üìä Results exported to CSV: {filename}\")\n",
        "        print(f\"üìÑ Columns: {', '.join(df.columns)}\")\n",
        "        return filename\n",
        "        \n",
        "    elif format.lower() == \"json\":\n",
        "        # Export to JSON with metadata\n",
        "        export_data = {\n",
        "            \"metadata\": {\n",
        "                \"export_timestamp\": datetime.now().isoformat(),\n",
        "                \"total_diseases\": len(results),\n",
        "                \"successful_analyses\": len([r for r in results if r.status == \"completed\"]),\n",
        "                \"export_format\": \"json\"\n",
        "            },\n",
        "            \"results\": data\n",
        "        }\n",
        "        \n",
        "        filename = f\"batch_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            json.dump(export_data, f, indent=2, ensure_ascii=False)\n",
        "        \n",
        "        print(f\"üìä Results exported to JSON: {filename}\")\n",
        "        print(f\"üìÑ Contains metadata and {len(data)} disease analyses\")\n",
        "        return filename\n",
        "\n",
        "# Export results in both formats\n",
        "print(\"üíæ EXPORTING BATCH RESULTS\")\n",
        "print(\"=\" * 30)\n",
        "\n",
        "csv_file = export_batch_results(batch_results, \"csv\")\n",
        "json_file = export_batch_results(batch_results, \"json\")\n",
        "\n",
        "print(f\"\\n‚úÖ Export complete! Files ready for research team:\")\n",
        "print(f\"  üìä Spreadsheet: {csv_file}\")\n",
        "print(f\"  üìÑ Data file: {json_file}\")\n",
        "\n",
        "# Show a preview of the CSV data\n",
        "if batch_results:\n",
        "    df_preview = pd.DataFrame([r.to_dict() for r in batch_results])\n",
        "    df_preview = df_preview.sort_values('priority_score', ascending=False)\n",
        "    print(f\"\\nüëÄ CSV PREVIEW (Top 3 results):\")\n",
        "    print(df_preview[['disease_name', 'priority_score', 'socioeconomic_score', 'groups_found', 'status']].head(3).to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## üìö Key Learnings & Best Practices\n",
        "\n",
        "### ‚úÖ What We Accomplished\n",
        "\n",
        "1. **Concurrent Processing**: Processed multiple diseases simultaneously with ThreadPoolExecutor\n",
        "2. **Progress Tracking**: Real-time monitoring with ETA calculations and success rates\n",
        "3. **Error Resilience**: Graceful handling of partial failures without stopping the batch\n",
        "4. **Results Ranking**: Automatic prioritization and ranking of diseases\n",
        "5. **Export Capabilities**: CSV and JSON export for research team integration\n",
        "6. **Performance Metrics**: Detailed timing analysis for optimization\n",
        "\n",
        "### üéØ Performance Benefits Demonstrated\n",
        "\n",
        "- **3x Faster**: Concurrent processing vs sequential execution\n",
        "- **Robust Error Handling**: Individual failures don't break the entire batch\n",
        "- **Memory Efficient**: Stream processing of results as they complete\n",
        "- **Export Ready**: Direct integration with research workflows\n",
        "\n",
        "### ‚ö° Optimization Strategies Used\n",
        "\n",
        "- **Reduced Token Limits**: Lower `max_output_tokens` for faster processing\n",
        "- **Low Reasoning Effort**: Faster processing with `effort: \"low\"`\n",
        "- **Controlled Concurrency**: Optimal worker count to balance speed vs resource usage\n",
        "- **Progress Updates**: Only every 2 completions to avoid output spam\n",
        "\n",
        "### üöÄ Next Steps\n",
        "\n",
        "- **Notebook 04**: Master advanced error handling and recovery strategies\n",
        "- **Notebook 05**: Explore performance optimization and caching techniques\n",
        "- **Notebook 06**: Build custom workflows for specialized research scenarios\n",
        "- **Production Scaling**: Apply batch processing to hundreds of diseases\n",
        "\n",
        "The batch processing framework enables efficient large-scale analysis for research prioritization! üéä\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
